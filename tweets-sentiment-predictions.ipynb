{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026396,
     "end_time": "2020-10-01T00:12:24.978872",
     "exception": false,
     "start_time": "2020-10-01T00:12:24.952476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This project is based on the dataset available at https://www.kaggle.com/c/tweet-sentiment-extraction/overview which is composed of about 20k tweets to train sentiment predictors.\n",
    "\n",
    "This notebook will guide you through the process of tweets cleaning (a very basic NLP task when dealing with text data), training a few Deep Learning models with different architectures and finally inferencing on test text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024527,
     "end_time": "2020-10-01T00:12:25.029846",
     "exception": false,
     "start_time": "2020-10-01T00:12:25.005319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:25.088360Z",
     "iopub.status.busy": "2020-10-01T00:12:25.087749Z",
     "iopub.status.idle": "2020-10-01T00:12:33.415394Z",
     "shell.execute_reply": "2020-10-01T00:12:33.415986Z"
    },
    "papermill": {
     "duration": 8.361376,
     "end_time": "2020-10-01T00:12:33.416187",
     "exception": false,
     "start_time": "2020-10-01T00:12:25.054811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024664,
     "end_time": "2020-10-01T00:12:33.467527",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.442863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:33.529657Z",
     "iopub.status.busy": "2020-10-01T00:12:33.528911Z",
     "iopub.status.idle": "2020-10-01T00:12:33.605902Z",
     "shell.execute_reply": "2020-10-01T00:12:33.605400Z"
    },
    "papermill": {
     "duration": 0.113268,
     "end_time": "2020-10-01T00:12:33.605998",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.492730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025738,
     "end_time": "2020-10-01T00:12:33.657753",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.632015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:33.724436Z",
     "iopub.status.busy": "2020-10-01T00:12:33.723863Z",
     "iopub.status.idle": "2020-10-01T00:12:33.734870Z",
     "shell.execute_reply": "2020-10-01T00:12:33.735296Z"
    },
    "papermill": {
     "duration": 0.051597,
     "end_time": "2020-10-01T00:12:33.735405",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.683808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16fab9f95b</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74a76f6e0a</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>DANGERously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04dd1d2e34</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bbe3cbf620</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going   \n",
       "1   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2   088c60f138                          my boss is bullying me...   \n",
       "3   9642c003ef                     what interview! leave me alone   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7   50e14c0bb8                                         Soooo high   \n",
       "8   e050245fbd                                        Both of you   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n",
       "11  16fab9f95b  I really really like the song Love Story by Ta...   \n",
       "12  74a76f6e0a       My Sharpie is running DANGERously low on ink   \n",
       "13  04dd1d2e34  i want to go to music tonight but i lost my vo...   \n",
       "14  bbe3cbf620                         test test from the LG enV2   \n",
       "\n",
       "                                        selected_text sentiment  \n",
       "0                 I`d have responded, if I were going   neutral  \n",
       "1                                            Sooo SAD  negative  \n",
       "2                                         bullying me  negative  \n",
       "3                                      leave me alone  negative  \n",
       "4                                       Sons of ****,  negative  \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                 fun  positive  \n",
       "7                                          Soooo high   neutral  \n",
       "8                                         Both of you   neutral  \n",
       "9                        Wow... u just became cooler.  positive  \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral  \n",
       "11                                               like  positive  \n",
       "12                                        DANGERously  negative  \n",
       "13                                               lost  negative  \n",
       "14                         test test from the LG enV2   neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:33.791113Z",
     "iopub.status.busy": "2020-10-01T00:12:33.790606Z",
     "iopub.status.idle": "2020-10-01T00:12:33.796521Z",
     "shell.execute_reply": "2020-10-01T00:12:33.796083Z"
    },
    "papermill": {
     "duration": 0.035269,
     "end_time": "2020-10-01T00:12:33.796612",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.761343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the dataset lenght\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:33.858372Z",
     "iopub.status.busy": "2020-10-01T00:12:33.857168Z",
     "iopub.status.idle": "2020-10-01T00:12:33.862446Z",
     "shell.execute_reply": "2020-10-01T00:12:33.861906Z"
    },
    "papermill": {
     "duration": 0.039715,
     "end_time": "2020-10-01T00:12:33.862582",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.822867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any other different value than neutral, negative and positive?\n",
    "train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:33.927343Z",
     "iopub.status.busy": "2020-10-01T00:12:33.926557Z",
     "iopub.status.idle": "2020-10-01T00:12:33.963001Z",
     "shell.execute_reply": "2020-10-01T00:12:33.962547Z"
    },
    "papermill": {
     "duration": 0.07261,
     "end_time": "2020-10-01T00:12:33.963088",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.890478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>7781</td>\n",
       "      <td>5861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "      <td>11117</td>\n",
       "      <td>11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>8582</td>\n",
       "      <td>5537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID   text  selected_text\n",
       "sentiment                              \n",
       "negative     7781   7781           5861\n",
       "neutral     11118  11117          11111\n",
       "positive     8582   8582           5537"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How's distributed the dataset? Is it biased?\n",
    "train.groupby('sentiment').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027055,
     "end_time": "2020-10-01T00:12:34.017453",
     "exception": false,
     "start_time": "2020-10-01T00:12:33.990398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Even when the dataset is a little bit biased, we'll keep it this way because the differences are not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:34.081882Z",
     "iopub.status.busy": "2020-10-01T00:12:34.080941Z",
     "iopub.status.idle": "2020-10-01T00:12:34.084345Z",
     "shell.execute_reply": "2020-10-01T00:12:34.084725Z"
    },
    "papermill": {
     "duration": 0.040234,
     "end_time": "2020-10-01T00:12:34.084834",
     "exception": false,
     "start_time": "2020-10-01T00:12:34.044600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's keep only the columns that we're going to use\n",
    "train = train[['selected_text','sentiment']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:34.147785Z",
     "iopub.status.busy": "2020-10-01T00:12:34.147011Z",
     "iopub.status.idle": "2020-10-01T00:12:34.150299Z",
     "shell.execute_reply": "2020-10-01T00:12:34.150719Z"
    },
    "papermill": {
     "duration": 0.038256,
     "end_time": "2020-10-01T00:12:34.150817",
     "exception": false,
     "start_time": "2020-10-01T00:12:34.112561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any null value?\n",
    "train[\"selected_text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:34.215409Z",
     "iopub.status.busy": "2020-10-01T00:12:34.214643Z",
     "iopub.status.idle": "2020-10-01T00:12:34.217654Z",
     "shell.execute_reply": "2020-10-01T00:12:34.217202Z"
    },
    "papermill": {
     "duration": 0.038861,
     "end_time": "2020-10-01T00:12:34.217757",
     "exception": false,
     "start_time": "2020-10-01T00:12:34.178896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's fill the only null value.\n",
    "train[\"selected_text\"].fillna(\"No content\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028485,
     "end_time": "2020-10-01T00:12:34.274863",
     "exception": false,
     "start_time": "2020-10-01T00:12:34.246378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The next steps about data cleaning will be:\n",
    "\n",
    "* Remove URLs from the tweets\n",
    "* Tokenize text\n",
    "* Remove emails\n",
    "* Remove new lines characters\n",
    "* Remove distracting single quotes\n",
    "* Remove all punctuation signs\n",
    "* Lowercase all text\n",
    "* Detokenize text\n",
    "* Convert list of texts to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:34.340698Z",
     "iopub.status.busy": "2020-10-01T00:12:34.339800Z",
     "iopub.status.idle": "2020-10-01T00:12:36.794791Z",
     "shell.execute_reply": "2020-10-01T00:12:36.795238Z"
    },
    "papermill": {
     "duration": 2.491816,
     "end_time": "2020-10-01T00:12:36.795384",
     "exception": false,
     "start_time": "2020-10-01T00:12:34.303568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing URLs with a regular expression\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "for i in range(len(train)):\n",
    "    train.at[i,'selected_text'] = remove_urls(train.iloc[i]['selected_text'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:36.862109Z",
     "iopub.status.busy": "2020-10-01T00:12:36.861360Z",
     "iopub.status.idle": "2020-10-01T00:12:36.863823Z",
     "shell.execute_reply": "2020-10-01T00:12:36.864371Z"
    },
    "papermill": {
     "duration": 0.039706,
     "end_time": "2020-10-01T00:12:36.864497",
     "exception": false,
     "start_time": "2020-10-01T00:12:36.824791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def depure_data(dataframe):\n",
    "    \n",
    "    #Tokenizing text\n",
    "    data = dataframe['selected_text'].values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    # Remove distracting commas\n",
    "    data = [re.sub(\",\", \"\", sent) for sent in data]\n",
    "\n",
    "    # making all lowercase\n",
    "    data = [sent.lower() for sent in data]\n",
    "\n",
    "    # Remove distracting dots\n",
    "    data = [sent.replace('.', '') for sent in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:36.941612Z",
     "iopub.status.busy": "2020-10-01T00:12:36.936456Z",
     "iopub.status.idle": "2020-10-01T00:12:37.213905Z",
     "shell.execute_reply": "2020-10-01T00:12:37.214379Z"
    },
    "papermill": {
     "duration": 0.320001,
     "end_time": "2020-10-01T00:12:37.214506",
     "exception": false,
     "start_time": "2020-10-01T00:12:36.894505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i`d have responded if i were going',\n",
       " 'sooo sad',\n",
       " 'bullying me',\n",
       " 'leave me alone',\n",
       " 'sons of ****']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = depure_data(train)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:37.292543Z",
     "iopub.status.busy": "2020-10-01T00:12:37.287270Z",
     "iopub.status.idle": "2020-10-01T00:12:38.215943Z",
     "shell.execute_reply": "2020-10-01T00:12:38.216454Z"
    },
    "papermill": {
     "duration": 0.9726,
     "end_time": "2020-10-01T00:12:38.216591",
     "exception": false,
     "start_time": "2020-10-01T00:12:37.243991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have', 'responded', 'if', 'were', 'going']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:38.280323Z",
     "iopub.status.busy": "2020-10-01T00:12:38.279547Z",
     "iopub.status.idle": "2020-10-01T00:12:38.283183Z",
     "shell.execute_reply": "2020-10-01T00:12:38.282742Z"
    },
    "papermill": {
     "duration": 0.03711,
     "end_time": "2020-10-01T00:12:38.283285",
     "exception": false,
     "start_time": "2020-10-01T00:12:38.246175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:38.366137Z",
     "iopub.status.busy": "2020-10-01T00:12:38.360950Z",
     "iopub.status.idle": "2020-10-01T00:12:40.919838Z",
     "shell.execute_reply": "2020-10-01T00:12:40.920449Z"
    },
    "papermill": {
     "duration": 2.607415,
     "end_time": "2020-10-01T00:12:40.920628",
     "exception": false,
     "start_time": "2020-10-01T00:12:38.313213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have responded if were going', 'sooo sad', 'bullying me', 'leave me alone', 'sons of', 'some shameless plugging for the best rangers forum on earth', 'fun', 'soooo high', 'both of you', 'wow just became cooler', 'as much as love to be hopeful reckon the chances are minimal never gonna get my cake and stuff', 'like', 'dangerously', 'lost', 'test test from the lg env']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(TreebankWordDetokenizer().detokenize(data_words[i]))\n",
    "print(data[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:41.006645Z",
     "iopub.status.busy": "2020-10-01T00:12:41.005912Z",
     "iopub.status.idle": "2020-10-01T00:12:41.008744Z",
     "shell.execute_reply": "2020-10-01T00:12:41.009172Z"
    },
    "papermill": {
     "duration": 0.055716,
     "end_time": "2020-10-01T00:12:41.009306",
     "exception": false,
     "start_time": "2020-10-01T00:12:40.953590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030582,
     "end_time": "2020-10-01T00:12:41.070729",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.040147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Label encoding\n",
    "\n",
    "As the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:41.159164Z",
     "iopub.status.busy": "2020-10-01T00:12:41.152988Z",
     "iopub.status.idle": "2020-10-01T00:12:41.162217Z",
     "shell.execute_reply": "2020-10-01T00:12:41.161787Z"
    },
    "papermill": {
     "duration": 0.060538,
     "end_time": "2020-10-01T00:12:41.162338",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.101800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = np.array(train['sentiment'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'neutral':\n",
    "        y.append(0)\n",
    "    if labels[i] == 'negative':\n",
    "        y.append(1)\n",
    "    if labels[i] == 'positive':\n",
    "        y.append(2)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:41.229007Z",
     "iopub.status.busy": "2020-10-01T00:12:41.228291Z",
     "iopub.status.idle": "2020-10-01T00:12:41.231497Z",
     "shell.execute_reply": "2020-10-01T00:12:41.231891Z"
    },
    "papermill": {
     "duration": 0.038976,
     "end_time": "2020-10-01T00:12:41.232001",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.193025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:41.298603Z",
     "iopub.status.busy": "2020-10-01T00:12:41.297939Z",
     "iopub.status.idle": "2020-10-01T00:12:41.302051Z",
     "shell.execute_reply": "2020-10-01T00:12:41.302452Z"
    },
    "papermill": {
     "duration": 0.039554,
     "end_time": "2020-10-01T00:12:41.302558",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.263004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030877,
     "end_time": "2020-10-01T00:12:41.364494",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.333617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data sequencing and splitting\n",
    "\n",
    "We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:41.433169Z",
     "iopub.status.busy": "2020-10-01T00:12:41.432417Z",
     "iopub.status.idle": "2020-10-01T00:12:42.472292Z",
     "shell.execute_reply": "2020-10-01T00:12:42.473546Z"
    },
    "papermill": {
     "duration": 1.078289,
     "end_time": "2020-10-01T00:12:42.473780",
     "exception": false,
     "start_time": "2020-10-01T00:12:41.395491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   68  145   41]\n",
      " [   0    0    0 ...    0  399   65]\n",
      " [   0    0    0 ...    0    0   11]\n",
      " ...\n",
      " [   0    0    0 ...  373   10    3]\n",
      " [   0    0    0 ...   24  540    4]\n",
      " [   0    0    0 ... 2401  197  651]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:42.608590Z",
     "iopub.status.busy": "2020-10-01T00:12:42.607773Z",
     "iopub.status.idle": "2020-10-01T00:12:42.611310Z",
     "shell.execute_reply": "2020-10-01T00:12:42.609274Z"
    },
    "papermill": {
     "duration": 0.068017,
     "end_time": "2020-10-01T00:12:42.611437",
     "exception": false,
     "start_time": "2020-10-01T00:12:42.543420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:42.735580Z",
     "iopub.status.busy": "2020-10-01T00:12:42.731234Z",
     "iopub.status.idle": "2020-10-01T00:12:42.746352Z",
     "shell.execute_reply": "2020-10-01T00:12:42.745890Z"
    },
    "papermill": {
     "duration": 0.085721,
     "end_time": "2020-10-01T00:12:42.746451",
     "exception": false,
     "start_time": "2020-10-01T00:12:42.660730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20610 6871 20610 6871\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,y_train, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056301,
     "end_time": "2020-10-01T00:12:42.836210",
     "exception": false,
     "start_time": "2020-10-01T00:12:42.779909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model building\n",
    "\n",
    "Alright, in the next cells I'll guide you through the process of building 3 Recurrent Neural Networks. I'll implement sequential models from the Keras API to achieve this task. Essentially, I'll start with a single layer **LSTM** network which is known by achieving good results in NLP tasks when the dataset is relatively small (I could have started with a SimpleRNN which is even simpler, but to be honest it's actually not deployed in production environments because it is too simple - however I'll leave it commented in case you want to know it's built). The next one will be a Bidirectional LSTM model, a more complex one and this particular one is known to achieve great metrics when talking about text classification. To go beyond the classic NLP approach, finally we'll implement a very unusual model: a Convolutional 1D network, known as well by delivering good metrics when talking about NLP. If everything goes ok, we should get the best results with the BidRNN, let's see what happens.\n",
    "\n",
    "Let's get hands on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050029,
     "end_time": "2020-10-01T00:12:42.934612",
     "exception": false,
     "start_time": "2020-10-01T00:12:42.884583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SimpleRNN model (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:43.050153Z",
     "iopub.status.busy": "2020-10-01T00:12:43.046049Z",
     "iopub.status.idle": "2020-10-01T00:12:43.053743Z",
     "shell.execute_reply": "2020-10-01T00:12:43.050750Z"
    },
    "papermill": {
     "duration": 0.069169,
     "end_time": "2020-10-01T00:12:43.053898",
     "exception": false,
     "start_time": "2020-10-01T00:12:42.984729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model0 = Sequential()\n",
    "#model0.add(layers.Embedding(max_words, 15))\n",
    "#model0.add(layers.SimpleRNN(15))\n",
    "#model0.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "#model0.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "#checkpoint0 = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "#history = model0.fit(X_train, y_train, epochs=5,validation_data=(X_test, y_test),callbacks=[checkpoint0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:12:43.155764Z",
     "iopub.status.busy": "2020-10-01T00:12:43.155134Z",
     "iopub.status.idle": "2020-10-01T00:21:28.035324Z",
     "shell.execute_reply": "2020-10-01T00:21:28.034856Z"
    },
    "papermill": {
     "duration": 524.92041,
     "end_time": "2020-10-01T00:21:28.035434",
     "exception": false,
     "start_time": "2020-10-01T00:12:43.115024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.8331 - accuracy: 0.6236\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.69844, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.8331 - accuracy: 0.6236 - val_loss: 0.7198 - val_accuracy: 0.6984\n",
      "Epoch 2/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.6485 - accuracy: 0.7416\n",
      "Epoch 00002: val_accuracy improved from 0.69844 to 0.75360, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.6485 - accuracy: 0.7416 - val_loss: 0.6036 - val_accuracy: 0.7536\n",
      "Epoch 3/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7767\n",
      "Epoch 00003: val_accuracy improved from 0.75360 to 0.79130, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.5599 - accuracy: 0.7768 - val_loss: 0.5326 - val_accuracy: 0.7913\n",
      "Epoch 4/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.5143 - accuracy: 0.8003\n",
      "Epoch 00004: val_accuracy improved from 0.79130 to 0.79886, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.5137 - accuracy: 0.8003 - val_loss: 0.5133 - val_accuracy: 0.7989\n",
      "Epoch 5/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8141\n",
      "Epoch 00005: val_accuracy improved from 0.79886 to 0.81051, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4855 - accuracy: 0.8139 - val_loss: 0.4880 - val_accuracy: 0.8105\n",
      "Epoch 6/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.8205\n",
      "Epoch 00006: val_accuracy improved from 0.81051 to 0.81648, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4712 - accuracy: 0.8205 - val_loss: 0.4767 - val_accuracy: 0.8165\n",
      "Epoch 7/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4548 - accuracy: 0.8279\n",
      "Epoch 00007: val_accuracy improved from 0.81648 to 0.81706, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4548 - accuracy: 0.8279 - val_loss: 0.4711 - val_accuracy: 0.8171\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4418 - accuracy: 0.8320\n",
      "Epoch 00008: val_accuracy improved from 0.81706 to 0.82099, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 12ms/step - loss: 0.4418 - accuracy: 0.8320 - val_loss: 0.4662 - val_accuracy: 0.8210\n",
      "Epoch 9/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.8334\n",
      "Epoch 00009: val_accuracy improved from 0.82099 to 0.82463, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4320 - accuracy: 0.8335 - val_loss: 0.4606 - val_accuracy: 0.8246\n",
      "Epoch 10/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8403\n",
      "Epoch 00010: val_accuracy improved from 0.82463 to 0.82652, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.4250 - accuracy: 0.8402 - val_loss: 0.4593 - val_accuracy: 0.8265\n",
      "Epoch 11/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.4194 - accuracy: 0.8419\n",
      "Epoch 00011: val_accuracy did not improve from 0.82652\n",
      "645/645 [==============================] - 7s 12ms/step - loss: 0.4190 - accuracy: 0.8421 - val_loss: 0.4570 - val_accuracy: 0.8254\n",
      "Epoch 12/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.4126 - accuracy: 0.8425\n",
      "Epoch 00012: val_accuracy did not improve from 0.82652\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4124 - accuracy: 0.8426 - val_loss: 0.4544 - val_accuracy: 0.8251\n",
      "Epoch 13/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.8477\n",
      "Epoch 00013: val_accuracy improved from 0.82652 to 0.82797, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.4046 - accuracy: 0.8477 - val_loss: 0.4488 - val_accuracy: 0.8280\n",
      "Epoch 14/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.4015 - accuracy: 0.8486\n",
      "Epoch 00014: val_accuracy improved from 0.82797 to 0.83001, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4014 - accuracy: 0.8488 - val_loss: 0.4455 - val_accuracy: 0.8300\n",
      "Epoch 15/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8521\n",
      "Epoch 00015: val_accuracy did not improve from 0.83001\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3969 - accuracy: 0.8522 - val_loss: 0.4481 - val_accuracy: 0.8296\n",
      "Epoch 16/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3918 - accuracy: 0.8553\n",
      "Epoch 00016: val_accuracy did not improve from 0.83001\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3921 - accuracy: 0.8551 - val_loss: 0.4387 - val_accuracy: 0.8294\n",
      "Epoch 17/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.8551\n",
      "Epoch 00017: val_accuracy improved from 0.83001 to 0.83190, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3867 - accuracy: 0.8551 - val_loss: 0.4489 - val_accuracy: 0.8319\n",
      "Epoch 18/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8581\n",
      "Epoch 00018: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3854 - accuracy: 0.8581 - val_loss: 0.4432 - val_accuracy: 0.8297\n",
      "Epoch 19/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3855 - accuracy: 0.8561\n",
      "Epoch 00019: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3854 - accuracy: 0.8562 - val_loss: 0.4494 - val_accuracy: 0.8223\n",
      "Epoch 20/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8577\n",
      "Epoch 00020: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3810 - accuracy: 0.8576 - val_loss: 0.4491 - val_accuracy: 0.8277\n",
      "Epoch 21/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8578\n",
      "Epoch 00021: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3819 - accuracy: 0.8580 - val_loss: 0.4441 - val_accuracy: 0.8300\n",
      "Epoch 22/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.8586\n",
      "Epoch 00022: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3789 - accuracy: 0.8586 - val_loss: 0.4472 - val_accuracy: 0.8307\n",
      "Epoch 23/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8576\n",
      "Epoch 00023: val_accuracy did not improve from 0.83190\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3789 - accuracy: 0.8576 - val_loss: 0.4423 - val_accuracy: 0.8312\n",
      "Epoch 24/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8601\n",
      "Epoch 00024: val_accuracy improved from 0.83190 to 0.83263, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3762 - accuracy: 0.8600 - val_loss: 0.4380 - val_accuracy: 0.8326\n",
      "Epoch 25/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8594\n",
      "Epoch 00025: val_accuracy did not improve from 0.83263\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3763 - accuracy: 0.8594 - val_loss: 0.4462 - val_accuracy: 0.8300\n",
      "Epoch 26/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3749 - accuracy: 0.8594\n",
      "Epoch 00026: val_accuracy did not improve from 0.83263\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3747 - accuracy: 0.8596 - val_loss: 0.4372 - val_accuracy: 0.8307\n",
      "Epoch 27/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8612\n",
      "Epoch 00027: val_accuracy did not improve from 0.83263\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3715 - accuracy: 0.8611 - val_loss: 0.4485 - val_accuracy: 0.8287\n",
      "Epoch 28/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3724 - accuracy: 0.8602\n",
      "Epoch 00028: val_accuracy did not improve from 0.83263\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3731 - accuracy: 0.8597 - val_loss: 0.4423 - val_accuracy: 0.8299\n",
      "Epoch 29/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8616\n",
      "Epoch 00029: val_accuracy improved from 0.83263 to 0.83350, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3702 - accuracy: 0.8615 - val_loss: 0.4371 - val_accuracy: 0.8335\n",
      "Epoch 30/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8618\n",
      "Epoch 00030: val_accuracy did not improve from 0.83350\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3689 - accuracy: 0.8616 - val_loss: 0.4342 - val_accuracy: 0.8331\n",
      "Epoch 31/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3689 - accuracy: 0.8616\n",
      "Epoch 00031: val_accuracy did not improve from 0.83350\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3686 - accuracy: 0.8618 - val_loss: 0.4365 - val_accuracy: 0.8331\n",
      "Epoch 32/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3639 - accuracy: 0.8650\n",
      "Epoch 00032: val_accuracy improved from 0.83350 to 0.83540, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3637 - accuracy: 0.8651 - val_loss: 0.4365 - val_accuracy: 0.8354\n",
      "Epoch 33/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3638 - accuracy: 0.8636\n",
      "Epoch 00033: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 12ms/step - loss: 0.3637 - accuracy: 0.8638 - val_loss: 0.4375 - val_accuracy: 0.8318\n",
      "Epoch 34/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8634\n",
      "Epoch 00034: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3625 - accuracy: 0.8633 - val_loss: 0.4377 - val_accuracy: 0.8338\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8643\n",
      "Epoch 00035: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3629 - accuracy: 0.8643 - val_loss: 0.4345 - val_accuracy: 0.8347\n",
      "Epoch 36/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8647\n",
      "Epoch 00036: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3604 - accuracy: 0.8647 - val_loss: 0.4382 - val_accuracy: 0.8312\n",
      "Epoch 37/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8683\n",
      "Epoch 00037: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3576 - accuracy: 0.8682 - val_loss: 0.4368 - val_accuracy: 0.8351\n",
      "Epoch 38/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.8684\n",
      "Epoch 00038: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3553 - accuracy: 0.8684 - val_loss: 0.4372 - val_accuracy: 0.8347\n",
      "Epoch 39/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3579 - accuracy: 0.8657\n",
      "Epoch 00039: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3578 - accuracy: 0.8657 - val_loss: 0.4324 - val_accuracy: 0.8354\n",
      "Epoch 40/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8662\n",
      "Epoch 00040: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3568 - accuracy: 0.8662 - val_loss: 0.4372 - val_accuracy: 0.8344\n",
      "Epoch 41/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8679\n",
      "Epoch 00041: val_accuracy did not improve from 0.83540\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3551 - accuracy: 0.8679 - val_loss: 0.4395 - val_accuracy: 0.8326\n",
      "Epoch 42/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3535 - accuracy: 0.8705\n",
      "Epoch 00042: val_accuracy improved from 0.83540 to 0.83554, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 12ms/step - loss: 0.3533 - accuracy: 0.8706 - val_loss: 0.4410 - val_accuracy: 0.8355\n",
      "Epoch 43/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8671\n",
      "Epoch 00043: val_accuracy improved from 0.83554 to 0.83641, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3564 - accuracy: 0.8671 - val_loss: 0.4355 - val_accuracy: 0.8364\n",
      "Epoch 44/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8697\n",
      "Epoch 00044: val_accuracy improved from 0.83641 to 0.83656, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3520 - accuracy: 0.8696 - val_loss: 0.4404 - val_accuracy: 0.8366\n",
      "Epoch 45/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3504 - accuracy: 0.8676\n",
      "Epoch 00045: val_accuracy did not improve from 0.83656\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3500 - accuracy: 0.8678 - val_loss: 0.4344 - val_accuracy: 0.8351\n",
      "Epoch 46/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8692\n",
      "Epoch 00046: val_accuracy did not improve from 0.83656\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3537 - accuracy: 0.8691 - val_loss: 0.4344 - val_accuracy: 0.8351\n",
      "Epoch 47/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3478 - accuracy: 0.8712\n",
      "Epoch 00047: val_accuracy did not improve from 0.83656\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3479 - accuracy: 0.8713 - val_loss: 0.4390 - val_accuracy: 0.8348\n",
      "Epoch 48/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.8710\n",
      "Epoch 00048: val_accuracy improved from 0.83656 to 0.83801, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3473 - accuracy: 0.8709 - val_loss: 0.4301 - val_accuracy: 0.8380\n",
      "Epoch 49/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8723\n",
      "Epoch 00049: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3476 - accuracy: 0.8721 - val_loss: 0.4395 - val_accuracy: 0.8376\n",
      "Epoch 50/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3534 - accuracy: 0.8662\n",
      "Epoch 00050: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3533 - accuracy: 0.8663 - val_loss: 0.4421 - val_accuracy: 0.8357\n",
      "Epoch 51/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3479 - accuracy: 0.8710\n",
      "Epoch 00051: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 8s 13ms/step - loss: 0.3477 - accuracy: 0.8711 - val_loss: 0.4312 - val_accuracy: 0.8374\n",
      "Epoch 52/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.8707\n",
      "Epoch 00052: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3459 - accuracy: 0.8707 - val_loss: 0.4357 - val_accuracy: 0.8369\n",
      "Epoch 53/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8728\n",
      "Epoch 00053: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3415 - accuracy: 0.8727 - val_loss: 0.4367 - val_accuracy: 0.8374\n",
      "Epoch 54/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.8726\n",
      "Epoch 00054: val_accuracy did not improve from 0.83801\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3448 - accuracy: 0.8727 - val_loss: 0.4344 - val_accuracy: 0.8380\n",
      "Epoch 55/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8713\n",
      "Epoch 00055: val_accuracy improved from 0.83801 to 0.83918, saving model to best_model1.hdf5\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3468 - accuracy: 0.8713 - val_loss: 0.4359 - val_accuracy: 0.8392\n",
      "Epoch 56/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.8705\n",
      "Epoch 00056: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 8s 13ms/step - loss: 0.3474 - accuracy: 0.8705 - val_loss: 0.4368 - val_accuracy: 0.8369\n",
      "Epoch 57/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3387 - accuracy: 0.8747\n",
      "Epoch 00057: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3387 - accuracy: 0.8747 - val_loss: 0.4367 - val_accuracy: 0.8345\n",
      "Epoch 58/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3433 - accuracy: 0.8728\n",
      "Epoch 00058: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3437 - accuracy: 0.8727 - val_loss: 0.4357 - val_accuracy: 0.8364\n",
      "Epoch 59/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.8742\n",
      "Epoch 00059: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 0.3412 - accuracy: 0.8743 - val_loss: 0.4355 - val_accuracy: 0.8373\n",
      "Epoch 60/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3425 - accuracy: 0.8733\n",
      "Epoch 00060: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 9s 14ms/step - loss: 0.3425 - accuracy: 0.8732 - val_loss: 0.4355 - val_accuracy: 0.8370\n",
      "Epoch 61/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3378 - accuracy: 0.8764\n",
      "Epoch 00061: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3375 - accuracy: 0.8765 - val_loss: 0.4345 - val_accuracy: 0.8389\n",
      "Epoch 62/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.8775\n",
      "Epoch 00062: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3359 - accuracy: 0.8775 - val_loss: 0.4382 - val_accuracy: 0.8366\n",
      "Epoch 63/70\n",
      "640/645 [============================>.] - ETA: 0s - loss: 0.3449 - accuracy: 0.8714\n",
      "Epoch 00063: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3452 - accuracy: 0.8714 - val_loss: 0.4410 - val_accuracy: 0.8352\n",
      "Epoch 64/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8757\n",
      "Epoch 00064: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3421 - accuracy: 0.8755 - val_loss: 0.4374 - val_accuracy: 0.8363\n",
      "Epoch 65/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.8751\n",
      "Epoch 00065: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 8s 13ms/step - loss: 0.3372 - accuracy: 0.8751 - val_loss: 0.4395 - val_accuracy: 0.8370\n",
      "Epoch 66/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8761\n",
      "Epoch 00066: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3375 - accuracy: 0.8761 - val_loss: 0.4436 - val_accuracy: 0.8364\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3372 - accuracy: 0.8758\n",
      "Epoch 00067: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3372 - accuracy: 0.8758 - val_loss: 0.4378 - val_accuracy: 0.8379\n",
      "Epoch 68/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.8766\n",
      "Epoch 00068: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3352 - accuracy: 0.8765 - val_loss: 0.4391 - val_accuracy: 0.8370\n",
      "Epoch 69/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3331 - accuracy: 0.8762\n",
      "Epoch 00069: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 8s 13ms/step - loss: 0.3331 - accuracy: 0.8762 - val_loss: 0.4402 - val_accuracy: 0.8374\n",
      "Epoch 70/70\n",
      "641/645 [============================>.] - ETA: 0s - loss: 0.3385 - accuracy: 0.8761\n",
      "Epoch 00070: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3387 - accuracy: 0.8760 - val_loss: 0.4394 - val_accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 15))\n",
    "model1.add(layers.LSTM(15,dropout=0.5))\n",
    "model1.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.348053,
     "end_time": "2020-10-01T00:21:32.974467",
     "exception": false,
     "start_time": "2020-10-01T00:21:30.626414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bidirectional LTSM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:21:37.632694Z",
     "iopub.status.busy": "2020-10-01T00:21:37.631560Z",
     "iopub.status.idle": "2020-10-01T00:36:23.410752Z",
     "shell.execute_reply": "2020-10-01T00:36:23.411175Z"
    },
    "papermill": {
     "duration": 888.114202,
     "end_time": "2020-10-01T00:36:23.411342",
     "exception": false,
     "start_time": "2020-10-01T00:21:35.297140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.8412 - accuracy: 0.6124\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.69801, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.8412 - accuracy: 0.6124 - val_loss: 0.7120 - val_accuracy: 0.6980\n",
      "Epoch 2/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.7444\n",
      "Epoch 00002: val_accuracy improved from 0.69801 to 0.77863, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.6361 - accuracy: 0.7444 - val_loss: 0.5682 - val_accuracy: 0.7786\n",
      "Epoch 3/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.5459 - accuracy: 0.7856\n",
      "Epoch 00003: val_accuracy improved from 0.77863 to 0.79683, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 13s 21ms/step - loss: 0.5458 - accuracy: 0.7856 - val_loss: 0.5209 - val_accuracy: 0.7968\n",
      "Epoch 4/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.5174 - accuracy: 0.7986\n",
      "Epoch 00004: val_accuracy improved from 0.79683 to 0.80396, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.5176 - accuracy: 0.7985 - val_loss: 0.5035 - val_accuracy: 0.8040\n",
      "Epoch 5/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4901 - accuracy: 0.8104\n",
      "Epoch 00005: val_accuracy improved from 0.80396 to 0.81531, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.4901 - accuracy: 0.8104 - val_loss: 0.4876 - val_accuracy: 0.8153\n",
      "Epoch 6/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.8165\n",
      "Epoch 00006: val_accuracy did not improve from 0.81531\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.4755 - accuracy: 0.8166 - val_loss: 0.4819 - val_accuracy: 0.8114\n",
      "Epoch 7/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.4629 - accuracy: 0.8221\n",
      "Epoch 00007: val_accuracy improved from 0.81531 to 0.81749, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 13s 19ms/step - loss: 0.4634 - accuracy: 0.8216 - val_loss: 0.4768 - val_accuracy: 0.8175\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4487 - accuracy: 0.8285\n",
      "Epoch 00008: val_accuracy improved from 0.81749 to 0.82186, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.4487 - accuracy: 0.8285 - val_loss: 0.4640 - val_accuracy: 0.8219\n",
      "Epoch 9/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.4398 - accuracy: 0.8329\n",
      "Epoch 00009: val_accuracy did not improve from 0.82186\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.4396 - accuracy: 0.8332 - val_loss: 0.4656 - val_accuracy: 0.8217\n",
      "Epoch 10/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.8341\n",
      "Epoch 00010: val_accuracy improved from 0.82186 to 0.82317, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.4322 - accuracy: 0.8342 - val_loss: 0.4578 - val_accuracy: 0.8232\n",
      "Epoch 11/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4294 - accuracy: 0.8368\n",
      "Epoch 00011: val_accuracy improved from 0.82317 to 0.82433, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.4293 - accuracy: 0.8368 - val_loss: 0.4559 - val_accuracy: 0.8243\n",
      "Epoch 12/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8423\n",
      "Epoch 00012: val_accuracy improved from 0.82433 to 0.82870, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.4225 - accuracy: 0.8424 - val_loss: 0.4455 - val_accuracy: 0.8287\n",
      "Epoch 13/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.4143 - accuracy: 0.8440\n",
      "Epoch 00013: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.4141 - accuracy: 0.8439 - val_loss: 0.4450 - val_accuracy: 0.8284\n",
      "Epoch 14/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4099 - accuracy: 0.8447\n",
      "Epoch 00014: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.4100 - accuracy: 0.8447 - val_loss: 0.4462 - val_accuracy: 0.8240\n",
      "Epoch 15/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8500\n",
      "Epoch 00015: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.4021 - accuracy: 0.8499 - val_loss: 0.4431 - val_accuracy: 0.8280\n",
      "Epoch 16/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8496\n",
      "Epoch 00016: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3994 - accuracy: 0.8496 - val_loss: 0.4477 - val_accuracy: 0.8284\n",
      "Epoch 17/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3978 - accuracy: 0.8519\n",
      "Epoch 00017: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.3978 - accuracy: 0.8519 - val_loss: 0.4464 - val_accuracy: 0.8287\n",
      "Epoch 18/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3953 - accuracy: 0.8529\n",
      "Epoch 00018: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3951 - accuracy: 0.8529 - val_loss: 0.4431 - val_accuracy: 0.8275\n",
      "Epoch 19/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8529\n",
      "Epoch 00019: val_accuracy did not improve from 0.82870\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3892 - accuracy: 0.8529 - val_loss: 0.4468 - val_accuracy: 0.8277\n",
      "Epoch 20/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8553\n",
      "Epoch 00020: val_accuracy improved from 0.82870 to 0.83409, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.3892 - accuracy: 0.8551 - val_loss: 0.4339 - val_accuracy: 0.8341\n",
      "Epoch 21/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8562\n",
      "Epoch 00021: val_accuracy did not improve from 0.83409\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.3854 - accuracy: 0.8562 - val_loss: 0.4367 - val_accuracy: 0.8334\n",
      "Epoch 22/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.8562\n",
      "Epoch 00022: val_accuracy did not improve from 0.83409\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3810 - accuracy: 0.8562 - val_loss: 0.4333 - val_accuracy: 0.8341\n",
      "Epoch 23/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8555\n",
      "Epoch 00023: val_accuracy did not improve from 0.83409\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.3830 - accuracy: 0.8553 - val_loss: 0.4395 - val_accuracy: 0.8320\n",
      "Epoch 24/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8584\n",
      "Epoch 00024: val_accuracy improved from 0.83409 to 0.83481, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3820 - accuracy: 0.8584 - val_loss: 0.4357 - val_accuracy: 0.8348\n",
      "Epoch 25/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8574\n",
      "Epoch 00025: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.3796 - accuracy: 0.8574 - val_loss: 0.4367 - val_accuracy: 0.8332\n",
      "Epoch 26/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.8590\n",
      "Epoch 00026: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3799 - accuracy: 0.8590 - val_loss: 0.4440 - val_accuracy: 0.8326\n",
      "Epoch 27/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8580\n",
      "Epoch 00027: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3782 - accuracy: 0.8582 - val_loss: 0.4421 - val_accuracy: 0.8336\n",
      "Epoch 28/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8601\n",
      "Epoch 00028: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3753 - accuracy: 0.8601 - val_loss: 0.4393 - val_accuracy: 0.8336\n",
      "Epoch 29/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8602\n",
      "Epoch 00029: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3779 - accuracy: 0.8602 - val_loss: 0.4373 - val_accuracy: 0.8329\n",
      "Epoch 30/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.8592\n",
      "Epoch 00030: val_accuracy did not improve from 0.83481\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3750 - accuracy: 0.8592 - val_loss: 0.4375 - val_accuracy: 0.8338\n",
      "Epoch 31/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3745 - accuracy: 0.8600\n",
      "Epoch 00031: val_accuracy improved from 0.83481 to 0.83612, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 15s 23ms/step - loss: 0.3748 - accuracy: 0.8599 - val_loss: 0.4355 - val_accuracy: 0.8361\n",
      "Epoch 32/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8608\n",
      "Epoch 00032: val_accuracy improved from 0.83612 to 0.83700, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3710 - accuracy: 0.8606 - val_loss: 0.4302 - val_accuracy: 0.8370\n",
      "Epoch 33/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8628\n",
      "Epoch 00033: val_accuracy did not improve from 0.83700\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3683 - accuracy: 0.8628 - val_loss: 0.4406 - val_accuracy: 0.8360\n",
      "Epoch 34/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3707 - accuracy: 0.8616\n",
      "Epoch 00034: val_accuracy improved from 0.83700 to 0.83889, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3707 - accuracy: 0.8617 - val_loss: 0.4353 - val_accuracy: 0.8389\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8607\n",
      "Epoch 00035: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3678 - accuracy: 0.8607 - val_loss: 0.4344 - val_accuracy: 0.8389\n",
      "Epoch 36/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3692 - accuracy: 0.8613\n",
      "Epoch 00036: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3693 - accuracy: 0.8613 - val_loss: 0.4335 - val_accuracy: 0.8386\n",
      "Epoch 37/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8602\n",
      "Epoch 00037: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3706 - accuracy: 0.8602 - val_loss: 0.4391 - val_accuracy: 0.8364\n",
      "Epoch 38/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8623\n",
      "Epoch 00038: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3672 - accuracy: 0.8622 - val_loss: 0.4324 - val_accuracy: 0.8361\n",
      "Epoch 39/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8645\n",
      "Epoch 00039: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3671 - accuracy: 0.8645 - val_loss: 0.4312 - val_accuracy: 0.8369\n",
      "Epoch 40/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8660\n",
      "Epoch 00040: val_accuracy did not improve from 0.83889\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3639 - accuracy: 0.8660 - val_loss: 0.4327 - val_accuracy: 0.8374\n",
      "Epoch 41/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.8653\n",
      "Epoch 00041: val_accuracy improved from 0.83889 to 0.83991, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3621 - accuracy: 0.8653 - val_loss: 0.4337 - val_accuracy: 0.8399\n",
      "Epoch 42/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3619 - accuracy: 0.8645\n",
      "Epoch 00042: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3621 - accuracy: 0.8644 - val_loss: 0.4338 - val_accuracy: 0.8355\n",
      "Epoch 43/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3616 - accuracy: 0.8639\n",
      "Epoch 00043: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3617 - accuracy: 0.8639 - val_loss: 0.4373 - val_accuracy: 0.8331\n",
      "Epoch 44/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3593 - accuracy: 0.8645\n",
      "Epoch 00044: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3595 - accuracy: 0.8644 - val_loss: 0.4316 - val_accuracy: 0.8367\n",
      "Epoch 45/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3567 - accuracy: 0.8690\n",
      "Epoch 00045: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3567 - accuracy: 0.8690 - val_loss: 0.4385 - val_accuracy: 0.8363\n",
      "Epoch 46/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8647\n",
      "Epoch 00046: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3587 - accuracy: 0.8646 - val_loss: 0.4319 - val_accuracy: 0.8379\n",
      "Epoch 47/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3582 - accuracy: 0.8667\n",
      "Epoch 00047: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3586 - accuracy: 0.8666 - val_loss: 0.4327 - val_accuracy: 0.8377\n",
      "Epoch 48/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3560 - accuracy: 0.8687\n",
      "Epoch 00048: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3561 - accuracy: 0.8687 - val_loss: 0.4345 - val_accuracy: 0.8395\n",
      "Epoch 49/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.8674\n",
      "Epoch 00049: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3581 - accuracy: 0.8674 - val_loss: 0.4396 - val_accuracy: 0.8338\n",
      "Epoch 50/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8671\n",
      "Epoch 00050: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3564 - accuracy: 0.8671 - val_loss: 0.4328 - val_accuracy: 0.8398\n",
      "Epoch 51/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8669\n",
      "Epoch 00051: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3585 - accuracy: 0.8669 - val_loss: 0.4319 - val_accuracy: 0.8396\n",
      "Epoch 52/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8657\n",
      "Epoch 00052: val_accuracy improved from 0.83991 to 0.84049, saving model to best_model2.hdf5\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3572 - accuracy: 0.8656 - val_loss: 0.4339 - val_accuracy: 0.8405\n",
      "Epoch 53/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3519 - accuracy: 0.8680\n",
      "Epoch 00053: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3518 - accuracy: 0.8680 - val_loss: 0.4336 - val_accuracy: 0.8398\n",
      "Epoch 54/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8675\n",
      "Epoch 00054: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3550 - accuracy: 0.8675 - val_loss: 0.4416 - val_accuracy: 0.8399\n",
      "Epoch 55/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3548 - accuracy: 0.8686\n",
      "Epoch 00055: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.3548 - accuracy: 0.8687 - val_loss: 0.4370 - val_accuracy: 0.8385\n",
      "Epoch 56/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.8698\n",
      "Epoch 00056: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3515 - accuracy: 0.8696 - val_loss: 0.4351 - val_accuracy: 0.8373\n",
      "Epoch 57/70\n",
      "642/645 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8672\n",
      "Epoch 00057: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3542 - accuracy: 0.8672 - val_loss: 0.4337 - val_accuracy: 0.8380\n",
      "Epoch 58/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8690\n",
      "Epoch 00058: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 14s 22ms/step - loss: 0.3529 - accuracy: 0.8690 - val_loss: 0.4319 - val_accuracy: 0.8379\n",
      "Epoch 59/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.8683\n",
      "Epoch 00059: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 13s 20ms/step - loss: 0.3546 - accuracy: 0.8681 - val_loss: 0.4395 - val_accuracy: 0.8386\n",
      "Epoch 60/70\n",
      "643/645 [============================>.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8682\n",
      "Epoch 00060: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3542 - accuracy: 0.8681 - val_loss: 0.4335 - val_accuracy: 0.8390\n",
      "Epoch 61/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8712\n",
      "Epoch 00061: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3517 - accuracy: 0.8712 - val_loss: 0.4418 - val_accuracy: 0.8396\n",
      "Epoch 62/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3470 - accuracy: 0.8700\n",
      "Epoch 00062: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3470 - accuracy: 0.8700 - val_loss: 0.4428 - val_accuracy: 0.8392\n",
      "Epoch 63/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8715\n",
      "Epoch 00063: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 19ms/step - loss: 0.3499 - accuracy: 0.8715 - val_loss: 0.4372 - val_accuracy: 0.8390\n",
      "Epoch 64/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8681\n",
      "Epoch 00064: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 15s 24ms/step - loss: 0.3501 - accuracy: 0.8681 - val_loss: 0.4364 - val_accuracy: 0.8392\n",
      "Epoch 65/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3446 - accuracy: 0.8723\n",
      "Epoch 00065: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3445 - accuracy: 0.8723 - val_loss: 0.4359 - val_accuracy: 0.8387\n",
      "Epoch 66/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.8707\n",
      "Epoch 00066: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3475 - accuracy: 0.8707 - val_loss: 0.4363 - val_accuracy: 0.8399\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.8698\n",
      "Epoch 00067: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3493 - accuracy: 0.8698 - val_loss: 0.4352 - val_accuracy: 0.8393\n",
      "Epoch 68/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3460 - accuracy: 0.8720\n",
      "Epoch 00068: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 12s 18ms/step - loss: 0.3460 - accuracy: 0.8721 - val_loss: 0.4414 - val_accuracy: 0.8342\n",
      "Epoch 69/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8715\n",
      "Epoch 00069: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 13s 21ms/step - loss: 0.3454 - accuracy: 0.8715 - val_loss: 0.4411 - val_accuracy: 0.8358\n",
      "Epoch 70/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.8731\n",
      "Epoch 00070: val_accuracy did not improve from 0.84049\n",
      "645/645 [==============================] - 14s 21ms/step - loss: 0.3426 - accuracy: 0.8731 - val_loss: 0.4475 - val_accuracy: 0.8331\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 20, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
    "model2.add(layers.Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6.268393,
     "end_time": "2020-10-01T00:36:35.500610",
     "exception": false,
     "start_time": "2020-10-01T00:36:29.232217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1D Convolutional model\n",
    "\n",
    "Before diving into this model, I know by prior experience that it tends to overfit extremely fast on small datasets. In this sense, just will implement it to show you how to do it in case it's of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:36:47.376632Z",
     "iopub.status.busy": "2020-10-01T00:36:47.375955Z",
     "iopub.status.idle": "2020-10-01T00:43:01.568289Z",
     "shell.execute_reply": "2020-10-01T00:43:01.339781Z"
    },
    "papermill": {
     "duration": 380.220093,
     "end_time": "2020-10-01T00:43:01.568454",
     "exception": false,
     "start_time": "2020-10-01T00:36:41.348361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 1.0424 - acc: 0.5585 - val_loss: 0.9078 - val_acc: 0.5989\n",
      "Epoch 2/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.8626 - acc: 0.6347 - val_loss: 0.8419 - val_acc: 0.6639\n",
      "Epoch 3/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.8039 - acc: 0.7187 - val_loss: 0.8107 - val_acc: 0.7095\n",
      "Epoch 4/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.7343 - acc: 0.7621 - val_loss: 0.7544 - val_acc: 0.7395\n",
      "Epoch 5/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.6535 - acc: 0.7953 - val_loss: 0.6620 - val_acc: 0.7904\n",
      "Epoch 6/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.6169 - acc: 0.8104 - val_loss: 0.6456 - val_acc: 0.7935\n",
      "Epoch 7/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.5888 - acc: 0.8208 - val_loss: 0.6493 - val_acc: 0.7909\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.5683 - acc: 0.8265 - val_loss: 0.6224 - val_acc: 0.7909\n",
      "Epoch 9/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.5484 - acc: 0.8312 - val_loss: 0.6710 - val_acc: 0.7658\n",
      "Epoch 10/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.5347 - acc: 0.8344 - val_loss: 0.6941 - val_acc: 0.7433\n",
      "Epoch 11/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.5228 - acc: 0.8387 - val_loss: 0.5910 - val_acc: 0.8019\n",
      "Epoch 12/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.5105 - acc: 0.8442 - val_loss: 0.6094 - val_acc: 0.8102\n",
      "Epoch 13/70\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.5014 - acc: 0.8492 - val_loss: 0.5954 - val_acc: 0.8149\n",
      "Epoch 14/70\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4909 - acc: 0.8541 - val_loss: 0.5816 - val_acc: 0.8021\n",
      "Epoch 15/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4810 - acc: 0.8557 - val_loss: 0.6094 - val_acc: 0.8059\n",
      "Epoch 16/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4752 - acc: 0.8576 - val_loss: 0.5940 - val_acc: 0.8133\n",
      "Epoch 17/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4680 - acc: 0.8591 - val_loss: 0.6045 - val_acc: 0.8152\n",
      "Epoch 18/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4636 - acc: 0.8608 - val_loss: 0.5803 - val_acc: 0.8141\n",
      "Epoch 19/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4574 - acc: 0.8641 - val_loss: 0.5746 - val_acc: 0.8118\n",
      "Epoch 20/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4531 - acc: 0.8643 - val_loss: 0.6988 - val_acc: 0.7468\n",
      "Epoch 21/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4476 - acc: 0.8675 - val_loss: 0.5751 - val_acc: 0.8112\n",
      "Epoch 22/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4438 - acc: 0.8668 - val_loss: 0.5706 - val_acc: 0.8168\n",
      "Epoch 23/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4387 - acc: 0.8720 - val_loss: 0.5934 - val_acc: 0.8104\n",
      "Epoch 24/70\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 0.4358 - acc: 0.8729 - val_loss: 0.5853 - val_acc: 0.8204\n",
      "Epoch 25/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4316 - acc: 0.8725 - val_loss: 0.5860 - val_acc: 0.8086\n",
      "Epoch 26/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.4283 - acc: 0.8756 - val_loss: 0.6175 - val_acc: 0.8169\n",
      "Epoch 27/70\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.4258 - acc: 0.8761 - val_loss: 0.5884 - val_acc: 0.8169\n",
      "Epoch 28/70\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.4214 - acc: 0.8788 - val_loss: 0.5889 - val_acc: 0.8123\n",
      "Epoch 29/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4178 - acc: 0.8776 - val_loss: 0.5791 - val_acc: 0.8200\n",
      "Epoch 30/70\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 0.4164 - acc: 0.8815 - val_loss: 0.6186 - val_acc: 0.7923\n",
      "Epoch 31/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4123 - acc: 0.8817 - val_loss: 0.5903 - val_acc: 0.8238\n",
      "Epoch 32/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4097 - acc: 0.8830 - val_loss: 0.6076 - val_acc: 0.8060\n",
      "Epoch 33/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4077 - acc: 0.8834 - val_loss: 0.5942 - val_acc: 0.8139\n",
      "Epoch 34/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4055 - acc: 0.8865 - val_loss: 0.5895 - val_acc: 0.8131\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4044 - acc: 0.8856 - val_loss: 0.5760 - val_acc: 0.8222\n",
      "Epoch 36/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.4003 - acc: 0.8867 - val_loss: 0.5976 - val_acc: 0.8112\n",
      "Epoch 37/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.3985 - acc: 0.8891 - val_loss: 0.5922 - val_acc: 0.8236\n",
      "Epoch 38/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3967 - acc: 0.8900 - val_loss: 0.5850 - val_acc: 0.8143\n",
      "Epoch 39/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3946 - acc: 0.8895 - val_loss: 0.5996 - val_acc: 0.8157\n",
      "Epoch 40/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.3935 - acc: 0.8915 - val_loss: 0.5948 - val_acc: 0.8166\n",
      "Epoch 41/70\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.3904 - acc: 0.8920 - val_loss: 0.6041 - val_acc: 0.8137\n",
      "Epoch 42/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.3878 - acc: 0.8915 - val_loss: 0.6383 - val_acc: 0.8102\n",
      "Epoch 43/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3863 - acc: 0.8941 - val_loss: 0.6487 - val_acc: 0.7951\n",
      "Epoch 44/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3837 - acc: 0.8938 - val_loss: 0.6268 - val_acc: 0.8032\n",
      "Epoch 45/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3835 - acc: 0.8951 - val_loss: 0.6261 - val_acc: 0.8131\n",
      "Epoch 46/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3803 - acc: 0.8968 - val_loss: 0.6344 - val_acc: 0.8194\n",
      "Epoch 47/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3795 - acc: 0.8977 - val_loss: 0.6140 - val_acc: 0.8038\n",
      "Epoch 48/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3801 - acc: 0.8967 - val_loss: 0.6366 - val_acc: 0.8141\n",
      "Epoch 49/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3766 - acc: 0.8988 - val_loss: 0.7050 - val_acc: 0.7546\n",
      "Epoch 50/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3751 - acc: 0.8984 - val_loss: 0.6113 - val_acc: 0.8102\n",
      "Epoch 51/70\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 0.3733 - acc: 0.8982 - val_loss: 0.6179 - val_acc: 0.8050\n",
      "Epoch 52/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3706 - acc: 0.9000 - val_loss: 0.6144 - val_acc: 0.8147\n",
      "Epoch 53/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3693 - acc: 0.9000 - val_loss: 0.6252 - val_acc: 0.8156\n",
      "Epoch 54/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.3677 - acc: 0.9026 - val_loss: 0.7046 - val_acc: 0.7703\n",
      "Epoch 55/70\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.3670 - acc: 0.9026 - val_loss: 0.6124 - val_acc: 0.8194\n",
      "Epoch 56/70\n",
      "645/645 [==============================] - 7s 10ms/step - loss: 0.3646 - acc: 0.9027 - val_loss: 0.6317 - val_acc: 0.8152\n",
      "Epoch 57/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3659 - acc: 0.9019 - val_loss: 0.6252 - val_acc: 0.8130\n",
      "Epoch 58/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3629 - acc: 0.9042 - val_loss: 0.6309 - val_acc: 0.8162\n",
      "Epoch 59/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3616 - acc: 0.9048 - val_loss: 0.6368 - val_acc: 0.8076\n",
      "Epoch 60/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3608 - acc: 0.9054 - val_loss: 0.6324 - val_acc: 0.8136\n",
      "Epoch 61/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3583 - acc: 0.9061 - val_loss: 0.6375 - val_acc: 0.8061\n",
      "Epoch 62/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3591 - acc: 0.9069 - val_loss: 0.6689 - val_acc: 0.8032\n",
      "Epoch 63/70\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 0.3562 - acc: 0.9074 - val_loss: 0.6442 - val_acc: 0.8139\n",
      "Epoch 64/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3567 - acc: 0.9050 - val_loss: 0.6508 - val_acc: 0.7989\n",
      "Epoch 65/70\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 0.3547 - acc: 0.9064 - val_loss: 0.7637 - val_acc: 0.7395\n",
      "Epoch 66/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3529 - acc: 0.9079 - val_loss: 0.6450 - val_acc: 0.7992\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.3520 - acc: 0.9083 - val_loss: 0.7405 - val_acc: 0.7439\n",
      "Epoch 68/70\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.3510 - acc: 0.9084 - val_loss: 0.6547 - val_acc: 0.8038\n",
      "Epoch 69/70\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.3495 - acc: 0.9087 - val_loss: 0.6649 - val_acc: 0.8114\n",
      "Epoch 70/70\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.3487 - acc: 0.9097 - val_loss: 0.6523 - val_acc: 0.8149\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "model3 = Sequential()\n",
    "model3.add(layers.Embedding(max_words, 128, input_length=max_len))\n",
    "model3.add(layers.Conv1D(18, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n",
    "model3.add(layers.MaxPooling1D(5))\n",
    "model3.add(layers.Conv1D(18, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n",
    "model3.add(layers.GlobalMaxPooling1D())\n",
    "model3.add(layers.Dense(3,activation='softmax'))\n",
    "model3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "checkpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model3.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7.643486,
     "end_time": "2020-10-01T00:43:17.406113",
     "exception": false,
     "start_time": "2020-10-01T00:43:09.762627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you check the val_accuracy metric in the training logs you won't find better score than the one achieved by the BidRNN. Again, the previous model is not the best for this task becaue is majorly used for short translation tasks, but the good thing to notice is its speed to train.\n",
    "\n",
    "Let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7.763914,
     "end_time": "2020-10-01T00:43:33.242123",
     "exception": false,
     "start_time": "2020-10-01T00:43:25.478209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Best model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:43:48.699062Z",
     "iopub.status.busy": "2020-10-01T00:43:48.698168Z",
     "iopub.status.idle": "2020-10-01T00:43:49.242577Z",
     "shell.execute_reply": "2020-10-01T00:43:49.241125Z"
    },
    "papermill": {
     "duration": 8.350385,
     "end_time": "2020-10-01T00:43:49.242690",
     "exception": false,
     "start_time": "2020-10-01T00:43:40.892305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's load the best model obtained during training\n",
    "best_model = keras.models.load_model(\"best_model2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:44:05.570209Z",
     "iopub.status.busy": "2020-10-01T00:44:05.569343Z",
     "iopub.status.idle": "2020-10-01T00:44:07.821340Z",
     "shell.execute_reply": "2020-10-01T00:44:07.820192Z"
    },
    "papermill": {
     "duration": 10.538696,
     "end_time": "2020-10-01T00:44:07.821453",
     "exception": false,
     "start_time": "2020-10-01T00:43:57.282757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 - 2s - loss: 0.4339 - accuracy: 0.8405\n",
      "Model accuracy:  0.8404890298843384\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:44:23.473678Z",
     "iopub.status.busy": "2020-10-01T00:44:23.472756Z",
     "iopub.status.idle": "2020-10-01T00:44:25.933265Z",
     "shell.execute_reply": "2020-10-01T00:44:25.932462Z"
    },
    "papermill": {
     "duration": 10.249555,
     "end_time": "2020-10-01T00:44:25.933381",
     "exception": false,
     "start_time": "2020-10-01T00:44:15.683826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7.799321,
     "end_time": "2020-10-01T00:44:41.224337",
     "exception": false,
     "start_time": "2020-10-01T00:44:33.425016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Alright, we all know the accuracy is not a good metric to measure how well a model is. That's the reason why I like to always see its confusion matrix, that way I have a better understanding of its classification and generalization ability. Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:44:56.733041Z",
     "iopub.status.busy": "2020-10-01T00:44:56.732196Z",
     "iopub.status.idle": "2020-10-01T00:44:56.743191Z",
     "shell.execute_reply": "2020-10-01T00:44:56.742727Z"
    },
    "papermill": {
     "duration": 7.685192,
     "end_time": "2020-10-01T00:44:56.743303",
     "exception": false,
     "start_time": "2020-10-01T00:44:49.058111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:45:13.187880Z",
     "iopub.status.busy": "2020-10-01T00:45:13.187187Z",
     "iopub.status.idle": "2020-10-01T00:45:13.496854Z",
     "shell.execute_reply": "2020-10-01T00:45:13.496398Z"
    },
    "papermill": {
     "duration": 8.454812,
     "end_time": "2020-10-01T00:45:13.496953",
     "exception": false,
     "start_time": "2020-10-01T00:45:05.042141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fccadccefd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "conf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n",
    "#Normalizing\n",
    "conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7.732896,
     "end_time": "2020-10-01T00:45:29.021368",
     "exception": false,
     "start_time": "2020-10-01T00:45:21.288472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, the model's score is very poor, but keep in mind it hasn't gone through hyperparameter tuning. Let's see how it performs on some test text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:45:44.666229Z",
     "iopub.status.busy": "2020-10-01T00:45:44.664774Z",
     "iopub.status.idle": "2020-10-01T00:45:44.667331Z",
     "shell.execute_reply": "2020-10-01T00:45:44.666875Z"
    },
    "papermill": {
     "duration": 7.761254,
     "end_time": "2020-10-01T00:45:44.667427",
     "exception": false,
     "start_time": "2020-10-01T00:45:36.906173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment = ['Neutral','Negative','Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:46:00.515238Z",
     "iopub.status.busy": "2020-10-01T00:46:00.514346Z",
     "iopub.status.idle": "2020-10-01T00:46:00.582421Z",
     "shell.execute_reply": "2020-10-01T00:46:00.583105Z"
    },
    "papermill": {
     "duration": 8.404853,
     "end_time": "2020-10-01T00:46:00.583320",
     "exception": false,
     "start_time": "2020-10-01T00:45:52.178467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this experience has been the worst , want my money back'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:46:16.584648Z",
     "iopub.status.busy": "2020-10-01T00:46:16.583746Z",
     "iopub.status.idle": "2020-10-01T00:46:16.627997Z",
     "shell.execute_reply": "2020-10-01T00:46:16.628683Z"
    },
    "papermill": {
     "duration": 7.958006,
     "end_time": "2020-10-01T00:46:16.628878",
     "exception": false,
     "start_time": "2020-10-01T00:46:08.670872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this is the best article i have read ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-01T00:46:32.143421Z",
     "iopub.status.busy": "2020-10-01T00:46:32.142577Z",
     "iopub.status.idle": "2020-10-01T00:46:32.187181Z",
     "shell.execute_reply": "2020-10-01T00:46:32.186361Z"
    },
    "papermill": {
     "duration": 8.037743,
     "end_time": "2020-10-01T00:46:32.187313",
     "exception": false,
     "start_time": "2020-10-01T00:46:24.149570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are annoying'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7.469766,
     "end_time": "2020-10-01T00:46:47.466134",
     "exception": false,
     "start_time": "2020-10-01T00:46:39.996368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We've reached the end of this notebook. I just wanted to highlight a few things before let you go.\n",
    "\n",
    "As you could see, very simple networks can achieve fantastic results. To go beyond, always the best approach is to build a model that underfit the data, then optimize it to overfit and finally start tuning your hyperparameters to achieve the metric that the business needs to reach. The way you tune the model is up to you, there's no magic formula for it, but adding regularization always works, as well as dropout. \n",
    "\n",
    "If you have any doubt, please feel free to comment :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 8.37058,
     "end_time": "2020-10-01T00:47:03.737723",
     "exception": false,
     "start_time": "2020-10-01T00:46:55.367143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2091.855212,
   "end_time": "2020-10-01T00:47:12.952802",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-01T00:12:21.097590",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
